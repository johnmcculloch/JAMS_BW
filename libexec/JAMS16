#!/usr/bin/env Rscript
suppressPackageStartupMessages(library(RCurl))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(optparse))
suppressPackageStartupMessages(library(futile.logger))
suppressPackageStartupMessages(library(parallel))
suppressPackageStartupMessages(library(benchmarkme))
#####################################
# Define System-specific Functions ##
#####################################

#Get slurm job ID
slurmjobid <- as.character(Sys.getenv("SLURM_JOB_ID"))

#Decide which kind of system you are on.
if(nchar(slurmjobid) < 3){
    cat("You are not on a Slurm Workload Cluster\n")
} else {
    cat(paste("You are on a Slurm Workload Manager Cluster under jobID", slurmjobid, "\n"))
}

#########################
# Get options from args #
#########################
#Define defaults
defopt <- list()
defopt$projname <- "MyProject"
defopt$outdir <- getwd()
defopt$classignore <- "N_A"
defopt$maxnumclasses <- 15
defopt$trunclen <- ("240,200")
defopt$taxamethod <- "dada2"
defopt$readcountthres <- 500
defopt$exporttables <- FALSE
defopt$plot_read_qualities <- FALSE
defopt$plotreadqual <- TRUE
#defopt$primerforw <- NULL
#defopt$primerrev <- NULL
#defopt$threads <- (as.numeric(detectHardwareResources()["threads"]))
#defopt$threads <- detectBatchCPUs()

option_list <- list(
    make_option(c("-p", "--projectname"),
                action="store", default = defopt$projname,
                type="character",
                help="Name of project to be used as prefix for output files (required)"),

    make_option(c("-o", "--outdir"),
                action="store", default = defopt$outdir,
                type="character",
                help ="Path to where you want the output."),

    make_option(c("-x", "--excel_metadata"),
                action="store", default = NULL,
                type="character",
                help="Metadata file in Excel format, with sheet_1 for phenotable and sheet_2 for phenolabels"),

    make_option(c("-t", "--phenotable_tsv"),
                action="store", default = NULL,
                type="character",
                help="Metadata file in tsv format, with headers"),

    make_option(c("-l", "--phenolabels_tsv"),
                action="store", default = NULL,
                type="character",
                help="Phenolabels file in tsv format, describing metadata file headers"),

    make_option(c("-c", "--maxnumclasses"),
                action="store", default = defopt$maxnumclasses,
                type="numeric",
                help="Maximum number of classes for a discrete variable to be kept. Default=15"),

    make_option(c("-i", "--class_to_ignore"),
                action="store", default = defopt$classignore,
                type="character",
                help="Ignore cells containing this text in the metadata. Samples bearing this class of text when comparing within a variable will be omitted from an analysis. Default = N_A"),

    make_option(c("-y", "--reads_files_path"),
                action="store", default = NULL,
                type="character",
                help ="path to where all fastq files are"),

    make_option(c("-b", "--basespace_projectID"), default = NULL,
                action="store",
                type="character",
                help = "Project ID for downloading reads from Illumina BaseSpace. If using the -b option, any reads from the -y input will be ignored. BaseSpace CLI must be installed. To install, see this: https://developer.basespace.illumina.com/docs/content/documentation/cli/cli-overview"),

    make_option(c("-T", "--trunclen_cutoffs_R1_R2"),
                action="store", default = defopt$trunclen,
                type="character",
                help ="Specify truncation lengths for R1,R2 (example: -T 240,200). If not specified, JAMS16 will impute appropriate values for truncation with fastq quality analysis."),

    make_option(c("-a", "--taxonomy_database"),
                action="store", default = NULL,
                type="character",
                help ="Path to the taxonomy database."),

    make_option(c("-M", "--taxamethod"),
                action="store", default=defopt$taxamethod,
                type="character",
                help = str_c("Choose analysis type (dada2, IDtaxa, dada2pr2, IDtaxapr2). First the program used to assign taxonomy is given (IDtaxa or dada2), then the database used for taxonomic assignment is given (SILVA or PR2), default: ", defopt$taxamethod, ")")),

    make_option(c("-u", "--read_count_threshold"),
                action="store", default = defopt$readcountthres,
                type="numeric",
                help ="path to the taxonomy database."),

    make_option(c("-e", "--export_tables"),
                action="store_true", default = defopt$exporttables,
                type="logical",
                help ="Export relative abundance tables and feature data tables in tsv format."),

#    make_option(c("-d", "--make_ordination_reports"),
#                action="store", default = NULL,
#                type="character",
#                help ="Generate ordination plots with this algorithm. Choose between PCA, tSNE or tUMAP, or more than one separated by a comma. Example: -d PCA,tUMAP"),

#    make_option(c("-f", "--make_exploratory_reports"),
#                action="store_true", default = FALSE,
#                type="logical",
#                help ="Generate Relative Abundace heatmaps and spreadsheets based on variance analysis."),

#    make_option(c("-k", "--make_comparative_reports"),
#                action="store_true", default = FALSE,
#                type="logical",
#                help ="Generate Relative Abundace heatmaps and spreadsheets for hypothesis testing using metadata categories."),

#    make_option(c("-r", "--make_correlation_reports"),
#                 action="store_true", default = FALSE,
#                type="logical",
#                help ="Generate Pairwise Feature Correlation heatmaps."),

#    make_option(c("-z", "--make_alpha_reports"),
#                action = "store_true", default = FALSE,
#                type = "logical",
#                help = "Generate alpha diversity plots."),
#
#    make_option(c("-m", "--plot_in_parallel"),
#                action = "store_true", default = FALSE,
#                type = "logical",
#                help = "Use multiple CPUs when generating plots. This option is experimental, and is not guaranteed to not crash."),

    make_option(c("-s", "--run_script"),
                action="store", default = NULL,
                type="character",
                help ="Path to an R script to run after all other options have been executed. Example: -s /path/to/make_these_specific_plots.R will execute the commands in this script at the end of JAMSbeta."),

    make_option(c("-Q", "--plot_read_qualities"),
                action = "store_true", default = defopt$plotreadqual,
                type = "logical",
                help = "Plot read qualities to PDF file. Default is to not plot."),

#    make_option(c("-F", "--forward_primer"),
#                action = "store", default = defopt$primerforw,
#                type = "character",
#                help = "Forward primer. Default is NULL."),

#    make_option(c("-R", "--reverse_primer"),
#                action = "store", default = defopt$primerrev,
#                type = "character",
#                help = "Reverse primer. Default is NULL."),

    make_option(c("-v", "--version"),
                action="store_true", help = "report version")
 )

# get the cmd line options to the script
args <- commandArgs(trailingOnly = TRUE)

# parse the options
opt <- parse_args(OptionParser(option_list = option_list), args)

#Add version from package to opt
opt$verstr <- paste0("JAMS v", packageVersion("JAMS"))

# print version & exit if -v
if (!is.null(opt$version)) {
    print(opt$verstr)
    quit()
}

#Load flogger package
#suppressMessages(suppressPackageStartupMessages(library(futile.logger)))

#Authorship message
authors <- as.character(as.person(packageDescription("JAMS")$Author))
authorshipmessage <- c(packageDescription("JAMS")$Title, paste("JAMS version", packageVersion("JAMS")), authors, paste("Contact:", "john.mcculloch@nih.gov"), "National Cancer Institute", "National Institutes of Health", "Bethesda, MD, USA")

#Fix path relativity
fixrelpath <- function(JAMSpath = NULL){
    require(R.utils)
    if (!(isAbsolutePath(JAMSpath))){
        fixedpath <- getAbsolutePath(JAMSpath)
    } else {
        fixedpath <- JAMSpath
    }
    #Chomp a "/" from the end of paths
    fixedpath <- gsub("/$", "", fixedpath)

    return(fixedpath)
}

for (pathtofix in c("outdir", "excel_metadata", "phenotable_tsv", "phenolabels_tsv", "reads_files_path", "taxonomy_database", "run_script")){
    if (!is.null(opt[[pathtofix]])){
        opt[[pathtofix]] <- fixrelpath(opt[[pathtofix]])
    }
}

if (!is.null(opt$run_script)){
    #Check that file exists
    if (!file.exists(opt$run_script)){
        flog.info(paste("Unable to find file", opt$run_script))
        q()
    }
}

#Abort if unreasonable instructions for taxonomy assignment
if (!(opt$taxamethod %in% c("dada2", "IDtaxa", "IDtaxapr2","dada2pr2"))){
    print("For an analysis chooose between one of these three options: dada2, IDtaxa, dada2pr2 or IDtaxapr2.")
    parse_args(OptionParser(option_list = option_list), c("-h"))
    q()
}

# Determine output directory
if (!is.null(opt$outdir)) {
    if (!file.exists(opt$outdir)){
        dir.create(opt$outdir, recursive = TRUE)
    }
    setwd(opt$outdir)
} else {
    opt$outdir <- getwd()
}

#Set defaults
asPA <- FALSE
opt$projimage <- file.path(opt$outdir, ".RData")

###################
## Start project ##
###################
suppressMessages(suppressPackageStartupMessages(library(JAMS)))
#The logfile is not an option as of a few JAMS versions ago. Therefore I commented out this loop
#if (TRUE){
#    opt$logfile <- file.path(opt$outdir, paste0(opt$projectname, "_JAMS.log"))
#    flog.appender(appender.tee(opt$logfile))
#}

flog.info("JAMS - Just A Microbiology System")
flog.info(authorshipmessage)

flog.info("JAMS16 start. Gathering information...")
project <- as.character(opt$projectname)
flog.info(paste("Using", project, "for name of project"))

opt$threads <- as.numeric(detectHardwareResources()["threads"])
flog.info(paste("You have", opt$threads, "CPUs available."))

flog.info(paste("Content will be output to", opt$outdir))

##################
## Get metadata ##
##################

###
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

#Get metadata
opt <- load_metadata_from_file(opt = opt)

#Stop if something went wrong
if ((is.null(opt$phenotable)) || (nrow(opt$phenotable) < 1)){
    stop("No data found in the metadata file supplied. Please check file.")
} else {
    flog.info(paste("Metadata has", ncol(opt$phenotable), "columns."))
}

##########
## Will keep this running in all cases. Not sure of the consequences for when loading a workspace and NOT changing metadata.
opt$metadatavetting <- TRUE

if (opt$metadatavetting == TRUE){
    #If phenolabels exists, make sure that phenotable contains only data described in phenolabels.
    if (!is.null(opt$phenolabels)){
        if (!all(colnames(opt$phenolabels) %in% c("Var_label", "Var_type"))) {
            stop("Could not find the Var_label and/or Var_type columns in phenolabels table. Consult documentation and check metadata file(s).")
        }

        validcols <- as.character(opt$phenolabels$Var_label)
        flog.info(paste("Phenolabels contains", length(validcols), "categories."))

        #Stop if you are asking for more than you have.
        if (length(validcols) > ncol(opt$phenotable)){
            stop("Phenolabels has more categories than the metadata in phenotable. Review and try again.")
        }

        ptsampcol <- as.character(opt$phenolabels$Var_label[which(opt$phenolabels$Var_type == "Sample")])
        if (length(ptsampcol) != 1){
             stop("You must define exactly one column as being the Sample column in your metadata. Please see documenation.")
        } else {
             flog.info(paste("Samples are in the", ptsampcol, "column in the metadata."))
        }

        flog.info(paste("Metadata classes specified in phenolabels are:", paste0(validcols, collapse = ", ")))
        flog.info("Adjusting metadata to contain only these columns.")

        #Stop if the classes you want in phenolabels do not exist in phenotable.
        if (all(validcols %in% colnames(opt$phenotable))){
            opt$phenotable <- opt$phenotable[ , validcols]
        } else {
            stop("One or more classes specified in phenolabels is missing as (a) column(s) in phenotable. Please check files.")
        }

    } else {
        #No phenolabels, so try and find at least a Sample column in the metadata.
        flog.info("Phenolabels not found. Will attempt to find which column is the Sample data.")
        ptsampcol <- colnames(opt$phenotable)[which(colnames(opt$phenotable) %in% c("Sample", "sample"))]
        if (length(ptsampcol) != 1){
            stop("You must define exactly one column as being the Sample column in your metadata. Please see documenation.")
        }
    }

    #Stop if something went wrong after subsetting phenotable to desired columns in phenolabels.
    if (nrow(opt$phenotable) < 1){
        stop("No data found in the metadata file supplied. Please check file.")
    }
}

#Save in case anything else goes wrong later
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

#Download BaseSpace reads, if required
if (!is.null(opt$basespace_projectID)){
    flog.info(paste("Downloading reads from BasesSpace under project ID", as.character(opt$basespace_projectID)))
    opt$reads_files_path <- file.path(opt$outdir, "BaseSpaceReads")
    dir.create(opt$reads_files_path, recursive = TRUE)
    cmd <- paste("bs", "download", "project", "-i", as.character(opt$basespace_projectID), "-o", opt$reads_files_path)
    system(cmd)
}

Samples_want <- as.character(opt$phenotable[ , ptsampcol])
#Look for corresponding fastqs with the wanted sample prefixes
#Get a list of fastq files
rawfastqpaths <- list.files(path = opt$reads_files_path, pattern = "fastq.gz", full.names = TRUE, recursive = TRUE)
rawfastqnames <- sapply(1:length(rawfastqpaths), function(x) { tail(unlist(strsplit(rawfastqpaths[x], split = "/")), n = 1) })
rawfastqsdf <- data.frame(Filenames = rawfastqnames, Filepaths = rawfastqpaths, stringsAsFactors = FALSE)
rawfastqsdf$Read <- sapply(1:nrow(rawfastqsdf), function(x) { analyze_fastq_filename(fn=rawfastqsdf$Filenames[x])$Read } )
rawfastqsdf$Lane <- sapply(1:nrow(rawfastqsdf), function(x) { analyze_fastq_filename(fn=rawfastqsdf$Filenames[x])$Lane } )
rawfastqsdf$OriPrefix <- sapply(1:nrow(rawfastqsdf), function(x) { analyze_fastq_filename(fn=rawfastqsdf$Filenames[x])$OriPrefix } )

#Subset them to wanted samples from metadata
Samples_have <- intersect(unique(rawfastqsdf$OriPrefix), unique(Samples_want))
#Report if anything is MISSING
if (length(Samples_have) < length(Samples_want)){
    flog.warn(paste("Fastq files for", (length(Samples_want) - length(Samples_have)), "of the samples on the metadata were not found in", opt$reads_files_path))
    flog.warn(paste("MISSING:", paste0(Samples_want[!(Samples_want %in% Samples_have)], collapse = ", ")))
    flog.warn("Check your metadata and fastq file path!")
}

#Subset fastqs to only the ones wanted from the metadata
rawfastqsdf <- subset(rawfastqsdf, OriPrefix %in% Samples_have)

#Check forward and reverse for each sample
Samples_have_R1R2 <- intersect(subset(rawfastqsdf, Read == "R1")[]$OriPrefix, subset(rawfastqsdf, Read == "R2")[]$OriPrefix)

if (length(Samples_have_R1R2) < length(Samples_have)){
    flog.warn(paste("Sample(s)", paste0(Samples_have[!(Samples_have %in% Samples_have_R1R2)], collapse = ", "), "is MISSING a file for one of the reads."))
    flog.warn("Check your metadata and fastq file path!")
}
rawfastqsdf <- subset(rawfastqsdf, OriPrefix %in% Samples_have_R1R2)

#Are there two samples?
if (nrow(rawfastqsdf) < 3){
    flog.warn(paste("There are only", nrow(rawfastqsdf), "samples that were found in the fastq file path. Check your metadata and fastq file path! Aborting now."))
}
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

#Check each sample has two files
if (length(Samples_have_R1R2) != (nrow(rawfastqsdf) / 2)){
    flog.warn("Some samples have more than two fastq files. Check your metadata and fastq file path! Aborting now.")
    q()
}
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

phredcutoff <- 18

if (is.null(opt$trunclen_cutoffs_R1_R2)){

    require("Rqc")
    flog.info("Analyzing read qualities")
    #Deal with the "all connections are in use" open by doing them in batches
    quality_list <- list()
    filelist <- split(1:length(rawfastqsdf$Filepaths), ceiling(seq_along(1:length(rawfastqsdf$Filepaths)) / 50))
    for (fls in 1:length(filelist)){
        quality_list_tmp <- suppressWarnings(Rqc::rqcQA(rawfastqsdf$Filepaths[filelist[[fls]]], workers = max((opt$threads - 2), 1)))
        quality_list <- c(quality_list, quality_list_tmp)
    }

    flog.info("Determining the best truncation length for reads")
    find_phred_cutoff <- function(qa = NULL, SampName = NULL){
        CyclePhreddf <- rqcCycleAverageQualityCalc(qa[[SampName]])
        CyclePhreddf$cycle <- as.numeric(CyclePhreddf$cycle)
        loess_out <- loess(quality ~ cycle, data = CyclePhreddf)
        CyclePhreddf$Fitted <- fitted(loess_out)
        cyclecutoffs <- sapply(10:30, function (x){ CyclePhreddf$cycle[max(which(CyclePhreddf$Fitted >= x))] })
        names(cyclecutoffs) <- paste0("Q", 10:30)

        return(cyclecutoffs)

    }

    CyclePhred_list <- lapply(names(quality_list), function(x){ find_phred_cutoff(qa = quality_list, SampName = x)} )

    names(CyclePhred_list) <- names(quality_list)
    CyclePhreddf_per_Sample <- as.data.frame(bind_rows(CyclePhred_list, .id = "id"))
    colnames(CyclePhreddf_per_Sample)[which(colnames(CyclePhreddf_per_Sample) == "id")] <- "Filenames"
    for (colm in 1:ncol(CyclePhreddf_per_Sample)){
        if (any(is.na(CyclePhreddf_per_Sample[ , colm]))){
            CyclePhreddf_per_Sample[is.na(CyclePhreddf_per_Sample[ , colm]), colm] <- 1
        }
    }

    rawfastqsdf <- left_join(rawfastqsdf, CyclePhreddf_per_Sample, by = "Filenames")
    rownames(rawfastqsdf) <- rawfastqsdf$Filenames

    quantile75cutoff_R1 <- sapply(paste0("Q", 10:30), function(x) { as.numeric(quantile(subset(rawfastqsdf, Read == "R1")[ , x], probs = seq(0, 1, 0.1))["90%"]) })
    quantile75cutoff_R2 <- sapply(paste0("Q", 10:30), function(x) { as.numeric(quantile(subset(rawfastqsdf, Read == "R2")[ , x], probs = seq(0, 1, 0.1))["90%"]) })

    trunclen_cutoffs_R1_R2 <- unname(c(quantile75cutoff_R1[paste0("Q", phredcutoff)], quantile75cutoff_R2[paste0("Q", phredcutoff)]))

    flog.info(paste("For R1 reads, on read cycle", trunclen_cutoffs_R1_R2[1], "the Phred quality is above", phredcutoff, "in the 90th percentile of reads"))
    flog.info(paste("For R2 reads, on read cycle", trunclen_cutoffs_R1_R2[2], "the Phred quality is above", phredcutoff, "in the 90th percentile of reads"))

} else {
    trunclen_cutoffs_R1_R2 <- as.numeric(unlist(strsplit(opt$trunclen_cutoffs_R1_R2, split = ",")))
    flog.info(paste("For R1 reads truncation length will be", trunclen_cutoffs_R1_R2[1], "as specified by the user"))
    flog.info(paste("For R2 reads truncation length will be", trunclen_cutoffs_R1_R2[2], "as specified by the user"))
}

#Flag samples which will not pass quality control
#flagR1_Samples <- rownames(subset(rawfastqsdf, Read == "R1"))[which(subset(rawfastqsdf, Read == "R1")[ , paste0("Q", phredcutoff)] <  trunclen_cutoffs_R1_R2[1])]
#flagR2_Samples <- rownames(subset(rawfastqsdf, Read == "R2"))[which(subset(rawfastqsdf, Read == "R2")[ , paste0("Q", phredcutoff)] <  trunclen_cutoffs_R1_R2[2])]

IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

##########################
## Start dada2 pipeline ##
##########################
#https://benjjneb.github.io/dada2/tutorial_1_8.html

flog.info("Starting dada2 pipeline according to https://benjjneb.github.io/dada2/tutorial_1_8.html")
sample.names <- subset(rawfastqsdf, Read == "R1")[]$OriPrefix

if (opt$plot_read_qualities){
    flog.info("Plotting raw read qualities using dada2")
    pdf(paste(paste(project, "Dada2_Read_Quality_Plots", sep = "_"), "pdf", sep = "."), paper = "a4r")
    for(smp in sample.names){
        print(plotQualityProfile(subset(rawfastqsdf, OriPrefix == smp)[order(subset(rawfastqsdf, OriPrefix == smp)$Read) , c("Filepaths")]))
    }
    dev.off()
}

rawfastqsdf$FilteredFilePaths <- sapply(1:nrow(rawfastqsdf), function(x){ file.path(opt$outdir, "filteredreads", paste0(paste(paste(rawfastqsdf[x , c("OriPrefix", "Read")], collapse = "_"), "filt", sep = "_"), ".fastq.gz")) })

#Be double sure to have the same order always
rawfastqsdfR1 <- subset(rawfastqsdf, Read == "R1")
rownames(rawfastqsdfR1) <- rawfastqsdfR1$OriPrefix
rawfastqsdfR2 <- subset(rawfastqsdf, Read == "R2")
rownames(rawfastqsdfR2) <- rawfastqsdfR2$OriPrefix
rawfastqsdfR2 <- rawfastqsdfR2[rownames(rawfastqsdfR1) , ]

flog.info("Filtering and trimming reads")
out <- filterAndTrim(fwd = rawfastqsdfR1$Filepaths, filt = rawfastqsdfR1$FilteredFilePaths, rev = rawfastqsdfR2$Filepaths, filt.rev = rawfastqsdfR2$FilteredFilePaths, truncLen = trunclen_cutoffs_R1_R2, compress = TRUE, truncQ = 2, maxN = 0, maxEE = c(2,2), rm.phix = TRUE, n = 1e+05, OMP = FALSE, verbose = TRUE, multithread = max((opt$threads - 2), 1))

#Adjust out dataframe
out <- as.data.frame(out)
out$Filenames <- rownames(out)
out <- left_join(out, rawfastqsdf[ , c("Filenames", "OriPrefix")], by = "Filenames")
colnames(out)[1:2] <- c("Number_Reads_Input", "Number_Reads_Output")
out$Filenames <- NULL
rawfastqsdf <- left_join(rawfastqsdf, out, by = "OriPrefix")

#Get a list of reads who do not survive read number threshold

samplestodiscard <- subset(out, Number_Reads_Output < opt$read_count_threshold)[]$OriPrefix
rawfastqsdf$Sample_Status <- "Sample_Kept_Read_Count_Sufficient"
if (length(samplestodiscard) > 0){
    rawfastqsdf$Sample_Status[which(rawfastqsdf$OriPrefix %in% samplestodiscard)] <- "Sample_Eliminated_Low_Read_Count"
    flog.warn("Samples", paste0(samplestodiscard, collapse = ", "), "were discarded for having less than", opt$read_count_threshold, "reads after filtering.")
    #Reset reads data frame as not containing reads to discard
    rawfastqsdfR1 <- subset(rawfastqsdfR1, !(OriPrefix %in% samplestodiscard))
    rawfastqsdfR2 <- subset(rawfastqsdfR2, !(OriPrefix %in% samplestodiscard))
}

Samples_passing_threshold <- intersect(rawfastqsdfR1$OriPrefix, rawfastqsdfR2$OriPrefix)

Sample2Depth <- out[ , c("OriPrefix", "Number_Reads_Output")]
colnames(Sample2Depth)[1] <- "Sample"

rm(out)

filtFs <- rawfastqsdfR1$FilteredFilePaths
filtRs <- rawfastqsdfR2$FilteredFilePaths

sample.names <- rawfastqsdfR1$OriPrefix
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)


flog.info("Learning errors")
errF <- learnErrors(filtFs, multithread = max((opt$threads - 2), 1))
errR <- learnErrors(filtRs, multithread = max((opt$threads - 2), 1))

pdf(paste(paste(project, "Dada2_Error_Rate_Plots", sep = "_"), "pdf", sep = "."), paper = "a4r")
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
dev.off()
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

flog.info("Dereplicating reads")
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)


flog.info("Running dada2 on dereplicated reads")
dadaFs <- dada(derepFs, err = errF, multithread = max((opt$threads - 2), 1))
dadaRs <- dada(derepRs, err = errR, multithread = max((opt$threads - 2), 1))
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

flog.info("Assembling amplicon contigs (\"merging pairs\")")
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE)
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

flog.info("Building counts table")
seqtab <- makeSequenceTable(mergers)
flog.info("Removing chimeras")
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus", multithread = max((opt$threads - 2), 1), verbose = TRUE)
##Adding rownames to seqtab.nochim which we need for expvec later if there is only one sample
if (length(sample.names) == 1){
    rownames(seqtab.nochim) <- sample.names
}
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

getN <- function(x) { sum(getUniques(x)) }
##this breaks here if there is only one sample so fixing it so one sample can run no problem
##Fixing track so that if there is only one sample things will still run
if (length(sample.names) == 1){
    track <- as.data.frame(cbind(getN(dadaFs), getN(dadaRs), getN(mergers), rowSums(seqtab.nochim)))
}else{
    track <- as.data.frame(cbind(sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim)))
}
colnames(track) <- c("DenoisedF", "DenoisedR", "Merged", "NonChim")
track$OriPrefix <- sample.names
rawfastqsdf <- left_join(rawfastqsdf, track, by = "OriPrefix")
flog.info("Exporting read stats")
write.xlsx(rawfastqsdf, file = paste0(opt$projectname, "_Reads_Statistics.xlsx"), colNames = TRUE, rowNames = FALSE, colWidths = "auto", borders = "all")

##########################
## Assigning Taxonomy ##
##########################
#Set training taxonomy depending for each method of taxonomy assignment
if (opt$taxamethod == "dada2"){
    opt$silva_trainset <- sort(file.path(opt$taxonomy_database, list.files(path = opt$taxonomy_database, pattern = "train_")))[1]
    opt$silva_species_assignment <- sort(file.path(opt$taxonomy_database, list.files(path = opt$taxonomy_database, pattern = "species")))[1]
}else if(opt$taxamethod == "IDtaxa"){
    opt$trainingset <- sort(file.path(opt$taxonomy_database, list.files(path = opt$taxonomy_database, pattern = glob2rx("SILVA*.RData"))))[1]
}else if(opt$taxamethod == "dada2pr2"){
    opt$trainingset <- sort(file.path(opt$taxonomy_database, list.files(path = opt$taxonomy_database, pattern = glob2rx("pr*.fasta.gz"))))[1]
}else if (opt$taxamethod == "IDtaxapr2"){
    opt$trainingset <- sort(file.path(opt$taxonomy_database, list.files(path = opt$taxonomy_database, pattern = glob2rx("pr*.rds"))))[1]
}

## IDtaxa with Silva ##
if (opt$taxamethod %in% c("IDtaxa")){
    flog.info("Obtaining DNAstringset from ASVs")
    dnafrommyseq <- DNAStringSet(getSequences(seqtab.nochim))
    flog.info("Assigning taxonomy with IDtaxa down to Species taxlevel. Please be patient")
    load(opt$trainingset)
    taxids <- IdTaxa(dnafrommyseq, trainingSet, strand="both", threshold = 60, processors=NULL, verbose=TRUE)
    ranks <- c("domain", "phylum", "class", "order", "family", "genus", "species")
    #Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
    taxid <- t(sapply(taxids, function(x) {
            m <- match(ranks, x$rank)
            taxa <- x$taxon[m]
            taxa[startsWith(taxa, "unclassified_")] <- NA
            taxa
    }))
    colnames(taxid) <- ranks; rownames(taxid) <- getSequences(seqtab.nochim)

    flog.info("Building JAMS-style SummarizedExperiment")
    #n.b. Pump all these objects into opt to declutter workspace.
    opt$tt <- as.data.frame(taxid)
    #Fix names to capital letters
    names(opt$tt) <- c("Domain", "Phylum", "Class", "Order", "Family", "Genus", "Species")
    opt$tt$Kingdom <- opt$tt$Domain
    opt$tt <- opt$tt[ , c("Domain", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")]
    taxlvls <- c("Domain", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus")
    taxtags <- c("d__", "k__", "p__", "c__", "o__", "f__", "g__")

## IDtaxa with PR2 ##
} else if (opt$taxamethod == "IDtaxapr2"){
flog.info("Obtaining DNAstringset from ASVs")
    dnafrommyseq <- DNAStringSet(getSequences(seqtab.nochim))
    flog.info("Assigning taxonomy with IDtaxa down to Species taxlevel. Please be patient")
    trainingSet <- readRDS(opt$trainingset)
    taxids <- IdTaxa(dnafrommyseq, trainingSet, strand="both", threshold = 60, processors=NULL, verbose=TRUE)
    IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)
    #No more Phylum and instead there is Supergroup, Division and Subdivision
    ranks <- c("Root","Domain","Supergroup","Division","Subdivision","Class","Order","Family","Genus","Species")
    #Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
    n_seq <- length(taxids)
    df_rows <- list()

    for(i in 1:n_seq){
        taxonomy<- taxids[[i]]$taxon
        apndlength= 10-length(taxonomy)
        taxonomy <- c(taxonomy,rep("Unclassifed",apndlength))
        #confidence <- taxids[[i]]$confidence
        #apndlength= 10-length(confidence)
        #confidence <- c(confidence,rep("100",apndlength))
        df_rows[[i]] = data.frame(taxonomy)
    }
    taxid <- t(as.data.frame(df_rows))
    colnames(taxid) <- ranks; rownames(taxid) <- getSequences(seqtab.nochim)

    flog.info("Building JAMS-style SummarizedExperiment")
    #n.b. Pump all these objects into opt to declutter workspace.
    #Fix names to capital letters
    opt$tt <- as.data.frame(taxid)
    opt$tt$Kingdom <- opt$tt$Domain
    opt$tt <- opt$tt[ , c("Domain", "Kingdom", "Supergroup","Division","Subdivision", "Class", "Order", "Family", "Genus", "Species")]
    taxlvls <- c("Domain", "Kingdom", "Supergroup", "Division", "Subdivision","Class", "Order", "Family", "Genus")
    taxtags <- c("d__", "k__", "sg__", "dv__", "sd__", "c__", "o__", "f__", "g__", "s__")

## DADA2 with PR2 ##
} else if (opt$taxamethod == "dada2pr2"){
    #Using Dada2 with PR2
    flog.info("Assigning taxonomy down to Species taxlevel with pr2")
    #No more Phylum and instead there is Supergroup, Division and Subdivision
    taxa <- assignTaxonomy(seqtab.nochim, opt$trainingset, taxLevels = c("Domain","Supergroup","Division","Subdivision","Class","Order","Family","Genus","Species"), multithread = max((opt$threads - 2), 1))
    IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

    flog.info("Building JAMS-style SummarizedExperiment")
    #n.b. Pump all these objects into opt to declutter workspace.
    opt$tt <- as.data.frame(taxa)
    #Fix names to capital letters
    names(opt$tt) <- c("Domain", "Supergroup", "Division", "Subdivision","Class", "Order", "Family", "Genus", "Species")
    opt$tt$Kingdom <- opt$tt$Domain
    opt$tt <- opt$tt[ , c("Domain", "Kingdom", "Supergroup","Division","Subdivision", "Class", "Order", "Family", "Genus", "Species")]
    taxlvls <- c("Domain", "Kingdom", "Supergroup", "Division", "Subdivision","Class", "Order", "Family", "Genus", "Species")
    taxtags <- c("d__", "k__", "sg__", "dv__", "sd__", "c__", "o__", "f__", "g__", "s__")


} else if (opt$taxamethod == "dada2"){
## DADA2 with Silva ##
    flog.info("Assigning taxonomy down to Genus taxlevel")
    taxa <- assignTaxonomy(seqtab.nochim, opt$silva_trainset, multithread = max((opt$threads - 2), 1))
    flog.info("Assigning taxonomy down to Species level")
    taxa <- addSpecies(taxa, opt$silva_species_assignment)
    IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

    flog.info("Building JAMS-style SummarizedExperiment")
    #n.b. Pump all these objects into opt to declutter workspace.
    opt$tt <- as.data.frame(taxa)
    opt$tt$Domain <- opt$tt$Kingdom
    opt$tt <- opt$tt[ , c("Domain", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")]
    taxlvls <- c("Domain", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus")
    taxtags <- c("d__", "k__", "p__", "c__", "o__", "f__", "g__")
}

IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

######################################
## Cleaning and Joining to Metadata ##
######################################

if(opt$taxamethod %in% c("dada2", "dada2pr2","IDtaxa")){
    for(lvl in 1:length(taxlvls)){
    taxlvl <- taxlvls[lvl]
    opt$tt[which(is.na(opt$tt[ , taxlvl]) == FALSE) , taxlvl] <- paste0(taxtags[lvl], opt$tt[which(is.na(opt$tt[ , taxlvl]) == FALSE) , taxlvl])
    opt$tt[which(is.na(opt$tt[ , taxlvl]) == TRUE) , taxlvl] <- "Unclassified"
    }
}
#Add genus-species to species column, like kraken does it so infer_LKT will work as is.
#tt[which(is.na(tt[ , "Species"]) == FALSE) , "Species"] <- paste(tt[which(is.na(tt[ , "Species"]) == FALSE), "Genus"], tt[which(is.na(tt[ , "Species"]) == FALSE) , "Species"], sep = "_")
#tt$Species <- gsub("^g__", "s__", tt$Species)

opt$tt[which(is.na(opt$tt[ , "Species"]) == TRUE) , "Species"] <- "Unclassified"
##Inferring LKT based on taxonomic database
if(opt$taxamethod %in% c("dada2","IDtaxa")){
    opt$tt <- infer_LKT(opt$tt)
    }else if (opt$taxamethod %in% c("dada2pr2","IDtaxapr2")){
    opt$tt <- infer_LKT_pr2(opt$tt)
    }

if(opt$taxamethod %in% c("IDtaxapr2")){
    for(lvl in 1:length(taxlvls)){
    taxlvl <- taxlvls[lvl]
    opt$tt[which(is.na(opt$tt[ , taxlvl]) == FALSE) , taxlvl] <- paste0(taxtags[lvl], opt$tt[which(is.na(opt$tt[ , taxlvl]) == FALSE) , taxlvl])
    }
}

opt$tt$Species <- opt$tt$LKT
opt$tt$LKT <- make.unique(opt$tt$LKT)

#Get counts table
opt$cts <- t(seqtab.nochim)
opt$cts <- as.data.frame(opt$cts)

#Maintain same order as cts
opt$tt <- opt$tt[rownames(opt$cts), ]

#Create ASV to LKT dictionary
asv2lkt <- data.frame(ASV = rownames(opt$tt), LKT = opt$tt$LKT, stringsAsFactors = FALSE)

#Call ASVs as non-redundant LKTs
rownames(opt$tt) <- opt$tt$LKT
rownames(opt$cts) <- opt$tt$LKT

rownames(opt$phenotable) <- opt$phenotable$Sample
pheno <- opt$phenotable

#Add sequencing depth
pheno <- left_join(pheno, Sample2Depth)

rownames(pheno) <- pheno$Sample

#Pheno for more than one sample.
if (length(sample.names) > 1){
    pheno <- pheno[colnames(opt$cts), ]
}

variable_list <- define_kinds_of_variables(phenolabels = opt$phenolabels, phenotable = pheno, maxclass = 15, maxsubclass = 4, class_to_ignore = "N_A", verbose = TRUE)
cdict <- make_colour_dictionary(variable_list = variable_list, pheno = pheno, class_to_ignore = "N_A", colour_of_class_to_ignore = "#bcc2c2", colour_table = NULL, shuffle = FALSE)

IO_jams_workspace_image(opt = opt, operation = "save", verbose = TRUE)

#Stop here and save if nothing else is required (if one sample only)
if (length(sample.names) == 1){
flog.info("Thank you for using JAMS16 for one sample")
    q()
}
##Otherwise build an expvec
expvec <- list()
expvec$LKT <- make_SummarizedExperiment_from_tables(pheno = pheno, counttable = opt$cts, featuretable = opt$tt, analysisname = "LKT")

metadata(expvec$LKT)$asv2lkt <- asv2lkt

#######
## Clean up workspace image to reduce RAM footprint

objects_to_clean <- c("dadaFs", "dadaRs", "derepFs", "derepRs", "mergers", "quality_list", "quality_list_tmp", "seqtab", "seqtab.nochim", "taxa", "asv2lkt", "colm", "errF", "errR", "filtFs", "filtRs", "fls")
objects_to_clean <- objects_to_clean[objects_to_clean %in% ls()]
rm(list = objects_to_clean)
gc()

#Save image
IO_jams_workspace_image(opt = opt, operation = "save", verbose = TRUE)

#Stop here and save if nothing else is required.
if (!any(c((!is.null(opt$make_ordination_reports)), (!is.null(opt$run_script)), opt$make_exploratory_reports, opt$make_comparative_reports, opt$make_PA_reports, opt$make_correlation_reports, opt$make_alpha_reports))){
    flog.info("Thank you for using JAMSbeta")
    q()
}

q()
