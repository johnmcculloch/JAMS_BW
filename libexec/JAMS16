#!/usr/bin/env Rscript
suppressPackageStartupMessages(library(RCurl))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(optparse))
suppressPackageStartupMessages(library(futile.logger))
suppressPackageStartupMessages(library(parallel))
suppressPackageStartupMessages(library(benchmarkme))
#####################################
# Define System-specific Functions ##
#####################################

#Get slurm job ID
slurmjobid <- as.character(Sys.getenv("SLURM_JOB_ID"))

#Decide which kind of system you are on.
if(nchar(slurmjobid) < 3){
    cat("You are not on a Slurm Workload Cluster\n")
} else {
    cat(paste("You are on a Slurm Workload Manager Cluster under jobID", slurmjobid, "\n"))
}

#########################
# Get options from args #
#########################
#Define defaults
defopt <- list()
defopt$projname <- "MyProject"
defopt$outdir <- getwd()
defopt$classignore <- "N_A"
defopt$maxnumclasses <- 15
defopt$trunclen <- ("240,200")
defopt$taxamethod <- "dada2"
defopt$min_read_count_threshold <- 500
defopt$exporttables <- FALSE
defopt$plot_read_qualities <- FALSE
defopt$primerforw <- NULL
defopt$primerrev <- NULL
#defopt$threads <- (as.numeric(detectHardwareResources()["threads"]))
#defopt$threads <- detectBatchCPUs()

option_list <- list(
    make_option(c("-p", "--projectname"),
                action="store", default = defopt$projname,
                type="character",
                help="Name of project to be used as prefix for output files (required)"),

    make_option(c("-o", "--outdir"),
                action="store", default = defopt$outdir,
                type="character",
                help ="Path to where you want the output."),

    make_option(c("-x", "--excel_metadata"),
                action="store", default = NULL,
                type="character",
                help="Metadata file in Excel format, with sheet_1 for phenotable and sheet_2 for phenolabels"),

    make_option(c("-t", "--phenotable_tsv"),
                action="store", default = NULL,
                type="character",
                help="Metadata file in tsv format, with headers"),

    make_option(c("-l", "--phenolabels_tsv"),
                action="store", default = NULL,
                type="character",
                help="Phenolabels file in tsv format, describing metadata file headers"),

    make_option(c("-c", "--maxnumclasses"),
                action="store", default = defopt$maxnumclasses,
                type="numeric",
                help="Maximum number of classes for a discrete variable to be kept. Default=15"),

    make_option(c("-i", "--class_to_ignore"),
                action="store", default = defopt$classignore,
                type="character",
                help="Ignore cells containing this text in the metadata. Samples bearing this class of text when comparing within a variable will be omitted from an analysis. Default = N_A"),

    make_option(c("-y", "--reads_files_path"),
                action="store", default = NULL,
                type="character",
                help ="path to where all fastq files are"),

    make_option(c("-b", "--basespace_projectID"), default = NULL,
                action="store",
                type="character",
                help = "Project ID for downloading reads from Illumina BaseSpace. If using the -b option, any reads from the -y input will be ignored. BaseSpace CLI must be installed. To install, see this: https://developer.basespace.illumina.com/docs/content/documentation/cli/cli-overview"),

    make_option(c("-T", "--trunclen_cutoffs_R1_R2"),
                action="store", default = defopt$trunclen,
                type="character",
                help ="Specify truncation lengths for R1,R2 (example: -T 240,200). If not specified, JAMS16 will impute appropriate values for truncation with fastq quality analysis."),

    make_option(c("-a", "--taxonomy_database"),
                action="store", default = NULL,
                type="character",
                help ="Path to the taxonomy database."),

    make_option(c("-M", "--taxamethod"),
                action="store", default=defopt$taxamethod,
                type="character",
                help = str_c("Choose analysis type (dada2, IDtaxa, dada2pr2, IDtaxapr2). First the program used to assign taxonomy is given (IDtaxa or dada2), then the database used for taxonomic assignment is given (SILVA or PR2), default: ", defopt$taxamethod, ")")),

    make_option(c("-u", "--min_read_count_threshold"),
                action="store", default = defopt$min_read_count_threshold,
                type="numeric",
                help = str_c("minimum number of reads for a sample to not be discarded. default: ", defopt$taxamethod, ")")),

    make_option(c("-n", "--max_read_count_threshold"),
                action="store", default = defopt$max_read_count_threshold,
                type="numeric",
                help = str_c("If set, reads will be capped to this number of reads. If not set, input fastqs will not be subsampled before DADA2.)")),

    make_option(c("-e", "--export_tables"),
                action="store_true", default = defopt$exporttables,
                type="logical",
                help ="Export relative abundance tables and feature data tables in tsv format."),

    make_option(c("-s", "--run_script"),
                action="store", default = NULL,
                type="character",
                help ="Path to an R script to run after all other options have been executed. Example: -s /path/to/make_these_specific_plots.R will execute the commands in this script at the end of JAMSbeta."),

    make_option(c("-Q", "--plot_read_qualities"),
                action = "store_true", default = defopt$plot_read_qualities,
                type = "logical",
                help = "Plot read qualities to PDF file. Default is to not plot."),

    make_option(c("-F", "--forward_primer"),
                action = "store", default = defopt$primerforw,
                type = "character",
                help = "Forward primer. Default is NULL."),

    make_option(c("-R", "--reverse_primer"),
                action = "store", default = defopt$primerrev,
                type = "character",
                help = "Reverse primer. Default is NULL."),

    make_option(c("-V", "--V4"),
                action="store_true", help = "use standard V4 primers GTGCCAGCMGCCGCGGTA and GGACTACHVGGGTWTCTAAT"),
    
    make_option(c("-v", "--version"),
                action="store_true", help = "report version")
 )

# get the cmd line options to the script
args <- commandArgs(trailingOnly = TRUE)

# parse the options
opt <- parse_args(OptionParser(option_list = option_list), args)

#Add version from package to opt
opt$verstr <- paste0("JAMS v", packageVersion("JAMS"))

# print version & exit if -v
if (!is.null(opt$version)) {
    print(opt$verstr)
    quit()
}

# set V4 primers
if (!is.null(opt$V4)) {
  opt$forward_primer <- "GTGCCAGCMGCCGCGGTA"
  opt$reverse_primer <- "GGACTACHVGGGTWTCTAA"
}

#Load flogger package
#suppressMessages(suppressPackageStartupMessages(library(futile.logger)))

#Authorship message
JAMSauthors <- as.character(as.person(packageDescription("JAMS")$Author))

#Fix path relativity
fixrelpath <- function(JAMSpath = NULL){
    require(R.utils)
    if (!(isAbsolutePath(JAMSpath))){
        fixedpath <- getAbsolutePath(JAMSpath)
    } else {
        fixedpath <- JAMSpath
    }
    #Chomp a "/" from the end of paths
    fixedpath <- gsub("/$", "", fixedpath)

    return(fixedpath)
}

for (pathtofix in c("outdir", "excel_metadata", "phenotable_tsv", "phenolabels_tsv", "reads_files_path", "taxonomy_database", "run_script")){
    if (!is.null(opt[[pathtofix]])){
        opt[[pathtofix]] <- fixrelpath(opt[[pathtofix]])
    }
}

if (!is.null(opt$run_script)){
    #Check that file exists
    if (!file.exists(opt$run_script)){
        flog.info(paste("Unable to find file", opt$run_script))
        q()
    }
}

#Abort if unreasonable instructions for taxonomy assignment
if (!(opt$taxamethod %in% c("dada2", "IDtaxa", "IDtaxapr2","dada2pr2"))){
    print("For an analysis chooose between one of these three options: dada2, IDtaxa, dada2pr2 or IDtaxapr2.")
    parse_args(OptionParser(option_list = option_list), c("-h"))
    q()
}

# Determine output directory
if (!is.null(opt$outdir)) {
    if (!file.exists(opt$outdir)){
        dir.create(opt$outdir, recursive = TRUE)
    }
    setwd(opt$outdir)
} else {
    opt$outdir <- getwd()
}

#Set defaults
asPA <- FALSE
opt$projimage <- file.path(opt$outdir, ".RData")

#Test slurm/Biowulf specific situation
if (is.null(opt$tempdir) & (file.exists(file.path("/lscratch", as.character(Sys.getenv("SLURM_JOB_ID")))))){
    opt$tempdir <- file.path("/lscratch", as.character(Sys.getenv("SLURM_JOB_ID")))
}

###################
## Start project ##
###################
suppressMessages(suppressPackageStartupMessages(library(JAMS)))
opt$logfile <- file.path(opt$outdir, paste0(opt$projectname, "_JAMS.log"))
flog.appender(appender.tee(opt$logfile))


flog.info("JAMS - Just A Microbiology System")
flog.info(paste("Version", opt$verstr))
flog.info(paste("By:", paste(JAMSauthors, collapse = "; ")))

flog.info("JAMS16 start. Gathering information...")
project <- as.character(opt$projectname)
flog.info(paste("Using", project, "for name of project"))

opt$threads <- as.numeric(detectHardwareResources()["threads"])
flog.info(paste("You have", opt$threads, "CPUs available."))

flog.info(paste("Content will be output to", opt$outdir))

##################
## Get metadata ##
##################

###
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

#Get metadata
opt <- load_metadata_from_file(opt = opt)

#Stop if something went wrong
if ((is.null(opt$phenotable)) || (nrow(opt$phenotable) < 1)){
    stop("No data found in the metadata file supplied. Please check file.")
} else {
    flog.info(paste("Metadata has", ncol(opt$phenotable), "columns."))
}

##########
## Will keep this running in all cases. Not sure of the consequences for when loading a workspace and NOT changing metadata.
opt$metadatavetting <- TRUE

if (opt$metadatavetting == TRUE){
    #If phenolabels exists, make sure that phenotable contains only data described in phenolabels.
    if (!is.null(opt$phenolabels)){
        if (!all(colnames(opt$phenolabels) %in% c("Var_label", "Var_type"))) {
            stop("Could not find the Var_label and/or Var_type columns in phenolabels table. Consult documentation and check metadata file(s).")
        }

        validcols <- as.character(opt$phenolabels$Var_label)
        flog.info(paste("Phenolabels contains", length(validcols), "categories."))

        #Stop if you are asking for more than you have.
        if (length(validcols) > ncol(opt$phenotable)){
            stop("Phenolabels has more categories than the metadata in phenotable. Review and try again.")
        }

        ptsampcol <- as.character(opt$phenolabels$Var_label[which(opt$phenolabels$Var_type == "Sample")])
        if (length(ptsampcol) != 1){
             stop("You must define exactly one column as being the Sample column in your metadata. Please see documenation.")
        } else {
             flog.info(paste("Samples are in the", ptsampcol, "column in the metadata."))
        }

        flog.info(paste("Metadata classes specified in phenolabels are:", paste0(validcols, collapse = ", ")))
        flog.info("Adjusting metadata to contain only these columns.")

        #Stop if the classes you want in phenolabels do not exist in phenotable.
        if (all(validcols %in% colnames(opt$phenotable))){
            opt$phenotable <- opt$phenotable[ , validcols]
        } else {
            stop("One or more classes specified in phenolabels is missing as (a) column(s) in phenotable. Please check files.")
        }

    } else {
        #No phenolabels, so try and find at least a Sample column in the metadata.
        flog.info("Phenolabels not found. Will attempt to find which column is the Sample data.")
        ptsampcol <- colnames(opt$phenotable)[which(colnames(opt$phenotable) %in% c("Sample", "sample"))]
        if (length(ptsampcol) != 1){
            stop("You must define exactly one column as being the Sample column in your metadata. Please see documenation.")
        }
    }

    #Stop if something went wrong after subsetting phenotable to desired columns in phenolabels.
    if (nrow(opt$phenotable) < 1){
        stop("No data found in the metadata file supplied. Please check file.")
    }
}

#Save in case anything else goes wrong later
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

#Download BaseSpace reads, if required
if (!is.null(opt$basespace_projectID)){
    flog.info(paste("Downloading reads from BasesSpace under project ID", as.character(opt$basespace_projectID)))
    opt$reads_files_path <- file.path(opt$outdir, "BaseSpaceReads")
    dir.create(opt$reads_files_path, recursive = TRUE)
    cmd <- paste("bs", "download", "project", "-i", as.character(opt$basespace_projectID), "-o", opt$reads_files_path)
    system(cmd)
}

Samples_want <- as.character(opt$phenotable[ , ptsampcol])

if (is.null(opt$reads_files_path)){
    if (!("SRAaccession" %in% colnames(opt$phenotable))){
        flog.warn("You must either supply a path to where your fastq files are located, or supply a column in your metatata named \"SRAaccession\" containing NCBI accession numbers for SRA amplicon runs. (https://www.ncbi.nlm.nih.gov/sra). You will need to have SRA toolkit installed in your system.")
        flog.warn("Aborting now.")
        q()
    } else {
        #SRA accessions are in a column
        flog.info("Retrieving fastq files from SRA. Please be patient.")
        opt$SRAaccessions <- opt$phenotable$SRAaccession
        opt$reads_files_path <- file.path(opt$outdir, "SRA_raw_reads")
        dir.create(opt$reads_files_path, recursive = TRUE)
        #Download from SRA into this folder
        download_SRA_reads(SRAaccessions = opt$SRAaccessions, outfolder = opt$reads_files_path, tempfolder = opt$tempdir, prefetch = FALSE, threads = opt$threads)
        #Rename reads to what is in the sample column
        setwd(opt$reads_files_path)
        for (rn in 1:nrow(opt$phenotable)){
            for (RN in c(1, 2)){
                if (file.exists(paste(opt$phenotable[rn, "SRAaccession"], paste(paste0("R", RN), "fastq", "gz", sep = "."), sep = "_"))){
                    file.rename(from = paste(opt$phenotable[rn, "SRAaccession"], paste(paste0("R", RN), "fastq", "gz", sep = "."), sep = "_"), to = paste(opt$phenotable[rn, "Sample"], paste(paste0("R", RN), "fastq.gz", sep = "."), sep = "_"))
                }
            }
        }
        setwd(opt$outdir)
    }
}

#Look for corresponding fastqs with the wanted sample prefixes
rawfastqpaths <- list.files(path = opt$reads_files_path, pattern = "fastq.gz", full.names = TRUE, recursive = TRUE)
rawfastqnames <- sapply(1:length(rawfastqpaths), function(x) { tail(unlist(strsplit(rawfastqpaths[x], split = "/")), n = 1) })
rawfastqsdf <- data.frame(Filenames = rawfastqnames, Filepaths = rawfastqpaths, stringsAsFactors = FALSE)
rawfastqsdf$Read <- sapply(1:nrow(rawfastqsdf), function(x) { analyze_fastq_filename(fn=rawfastqsdf$Filenames[x])$Read } )
rawfastqsdf$Lane <- sapply(1:nrow(rawfastqsdf), function(x) { analyze_fastq_filename(fn=rawfastqsdf$Filenames[x])$Lane } )
rawfastqsdf$OriPrefix <- sapply(1:nrow(rawfastqsdf), function(x) { analyze_fastq_filename(fn=rawfastqsdf$Filenames[x])$OriPrefix } )
if ("OriPrefix" %in% colnames(opt$phenotable)){
    #Match OriPrefix to Sample
    #ADD THIS LATER
} else {
    #Consider Sample is fastq prefix name
    rawfastqsdf$Prefix <- rawfastqsdf$OriPrefix
}

#Subset them to wanted samples from metadata
Samples_have <- intersect(unique(rawfastqsdf$Prefix), unique(Samples_want))
#Report if anything is MISSING
if (length(Samples_have) < length(Samples_want)){
    flog.warn(paste("Fastq files for", (length(Samples_want) - length(Samples_have)), "of the samples on the metadata were not found in", opt$reads_files_path))
    flog.warn(paste("MISSING:", paste0(Samples_want[!(Samples_want %in% Samples_have)], collapse = ", ")))
    flog.warn("Check your metadata and fastq file path!")
}

#Subset fastqs to only the ones wanted from the metadata
rawfastqsdf <- subset(rawfastqsdf, Prefix %in% Samples_have)

#Check forward and reverse for each sample
Samples_have_R1R2 <- intersect(subset(rawfastqsdf, Read == "R1")[]$Prefix, subset(rawfastqsdf, Read == "R2")[]$Prefix)

if (length(Samples_have_R1R2) < length(Samples_have)){
    flog.warn(paste("Sample(s)", paste0(Samples_have[!(Samples_have %in% Samples_have_R1R2)], collapse = ", "), "is MISSING a file for one of the reads."))
    flog.warn("Check your metadata and fastq file path!")
}
rawfastqsdf <- subset(rawfastqsdf, Prefix %in% Samples_have_R1R2)

#Check each sample has two files
if (length(Samples_have_R1R2) != (nrow(rawfastqsdf) / 2)){
    flog.warn("Some samples have more than two fastq files. Check your metadata and fastq file path! Aborting now.")
    IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)
    q()
}

#Are there two samples?
if (nrow(rawfastqsdf) < 3){
    flog.warn(paste("There are only", nrow(rawfastqsdf), "samples that were found in the fastq file path. Check your metadata and fastq file path! Aborting now."))
    IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)
    q()
}

#Set column names which will be used as input for DADA2. If subsampling reads, as below, files on these columns might change.
rawfastqsdf$Dada2_input_read_fpath <- rawfastqsdf$Filepaths
rawfastqsdf$Dada2_input_read_fn <- rawfastqsdf$Filenames

#Cap to a set number of reads, if required.
if (!is.null(opt$max_read_count_threshold)){
    flog.info(paste("Input reads will be assessed and capped to", opt$max_read_count_threshold, "if found to contain more reads than this."))
    #Check you have seqtk on the system.
    missingdep <- FALSE
    dependencies_to_check <- "seqtk"
    for (dep in dependencies_to_check) {
        cmd <- dep
        if (cmd == "sratoolkit"){
            cmd <- "fasterq-dump"
        } 
        if (system(str_c("which ", cmd), ignore.stdout = TRUE) == 1) { # not found
            flog.warn(str_c("You are missing ", dep))
            missingdep <- TRUE
        }
    }

    if (missingdep == TRUE) {
        flog.info("Please install missing dependencies before using JAMS16. Aborting.")
        opt$abort <- TRUE
        q()
    }

    #Check number of reads before starting
    rawfastqstats <- countfastq_files(fastqfiles = rawfastqsdf$Filepaths, threads = opt$threads, type_tag = "Raw")
    colnames(rawfastqstats)[which(colnames(rawfastqstats) == "Reads")] <- "Filepaths"
    rawfastqstats <- left_join(rawfastqstats, rawfastqsdf[ , c("Filepaths", "Read", "Prefix")], by = "Filepaths")
    rawfastqstats$Status <- "OK"
    if (any(rawfastqstats$Count > opt$max_read_count_threshold)){
        rawfastqstats[which(rawfastqstats$Count > opt$max_read_count_threshold), "Status"] <- "High"
    }

    #Subsample reads if applicable.
    if ("High" %in% rawfastqstats$Status){
        #Create folder to hold subsamples reads.
        subsampled_readsdir <- file.path(opt$outdir, "subsampled_reads")
        dir.create(subsampled_readsdir, recursive = TRUE)
        rawfastqstats$Subset_out_fn <- NA
        rawfastqstats$Dada2_input_read_fn <- NA
        rawfastqstats$Dada2_input_read_fpath <- rawfastqstats$Filepaths
        for (rn in which(rawfastqstats$Status == "High")){
            rawfastqstats[rn, "Subset_out_fn"] <- file.path(subsampled_readsdir, paste(paste(rawfastqstats[rn, "Prefix"], paste0("subs", opt$max_read_count_threshold), rawfastqstats[rn, "Read"], sep = "_"), "fastq", sep = "."))
            rawfastqstats[rn, "Dada2_input_read_fn"] <- paste(paste(rawfastqstats[rn, "Prefix"], paste0("subs", opt$max_read_count_threshold), rawfastqstats[rn, "Read"], sep = "_"), "fastq", "gz", sep = ".")
            rawfastqstats[rn, "Dada2_input_read_fpath"] <- file.path(subsampled_readsdir, paste(paste(rawfastqstats[rn, "Prefix"], paste0("subs", opt$max_read_count_threshold), rawfastqstats[rn, "Read"], sep = "_"), "fastq", "gz", sep = "."))
        }

        subsample_and_compress <- function(inputfastq = NULL, numreads = NULL, outputfastq = NULL){
            subsampleargs <- c("sample", "-s100", inputfastq, numreads, ">", outputfastq)
            system2('seqtk', args = subsampleargs)
            #Compress to gz because seqtk uncompresses them.
            system2('gzip', args = outputfastq)
        }

        dmp <- mclapply(which(rawfastqstats$Status == "High"), function (x) { subsample_and_compress(inputfastq = rawfastqstats[x, "Filepaths"], numreads = opt$max_read_count_threshold, outputfastq = rawfastqstats[x, "Subset_out_fn"]) }, mc.cores = opt$threads)
        rm(dmp)

        #Rectify file paths on rawfastqsdf to reflect subsetted samples.
        rownames(rawfastqsdf) <- rawfastqsdf$Filepaths
        rownames(rawfastqstats) <- rawfastqstats$Filepaths
        rawfastqsdf[rownames(rawfastqstats)[which(rawfastqstats$Status == "High")], "Dada2_input_read_fn"] <- rawfastqstats[which(rawfastqstats$Status == "High"), "Dada2_input_read_fn"]
        rawfastqsdf[rownames(rawfastqstats)[which(rawfastqstats$Status == "High")], "Dada2_input_read_fpath"] <- rawfastqstats[which(rawfastqstats$Status == "High"), "Dada2_input_read_fpath"]
        rownames(rawfastqsdf) <- 1:nrow(rawfastqsdf)
        rownames(rawfastqstats) <- 1:nrow(rawfastqstats)
    }
}

phredcutoff <- 18

if (is.null(opt$trunclen_cutoffs_R1_R2)){

    #To be developed.
    trunclen_cutoffs_R1_R2 <- c(250, 250)

} else {
    trunclen_cutoffs_R1_R2 <- as.numeric(unlist(strsplit(opt$trunclen_cutoffs_R1_R2, split = ",")))
    flog.info(paste("For R1 reads truncation length will be", trunclen_cutoffs_R1_R2[1], "as specified by the user"))
    flog.info(paste("For R2 reads truncation length will be", trunclen_cutoffs_R1_R2[2], "as specified by the user"))
}

#Flag samples which will not pass quality control
#flagR1_Samples <- rownames(subset(rawfastqsdf, Read == "R1"))[which(subset(rawfastqsdf, Read == "R1")[ , paste0("Q", phredcutoff)] <  trunclen_cutoffs_R1_R2[1])]
#flagR2_Samples <- rownames(subset(rawfastqsdf, Read == "R2"))[which(subset(rawfastqsdf, Read == "R2")[ , paste0("Q", phredcutoff)] <  trunclen_cutoffs_R1_R2[2])]

IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

##########################
## Start dada2 pipeline ##
##########################
#https://benjjneb.github.io/dada2/tutorial_1_8.html

flog.info("Starting dada2 pipeline according to https://benjjneb.github.io/dada2/tutorial_1_8.html")
sample.names <- subset(rawfastqsdf, Read == "R1")[]$Prefix

if (opt$plot_read_qualities){
    flog.info("Plotting raw read qualities using dada2")
    pdf(paste(paste(project, "Dada2_Read_Quality_Plots", sep = "_"), "pdf", sep = "."), paper = "a4r")
    for(smp in sample.names){
        print(plotQualityProfile(subset(rawfastqsdf, Prefix == smp)[order(subset(rawfastqsdf, Prefix == smp)$Read) , c("Dada2_input_read_fn")]))
    }
    dev.off()
}

rawfastqsdf$FilteredFilePaths <- sapply(1:nrow(rawfastqsdf), function(x){ file.path(opt$outdir, "filteredreads", paste0(paste(paste(rawfastqsdf[x , c("Prefix", "Read")], collapse = "_"), "filt", sep = "_"), ".fastq.gz")) })

if (!is.null(opt$forward_primer) && !is.null(opt$reverse_primer)) {
    rawfastqsdf$PrimerClippedFilePaths <- sapply(1:nrow(rawfastqsdf), function(x){ file.path(opt$outdir, "primerremoved", paste0(paste(paste(rawfastqsdf[x , c("Prefix", "Read")], collapse = "_"), "primerremoved", sep = "_"), ".fastq.gz")) })
    rawfastqsdf$FilterInput_fn <-  sapply(1:nrow(rawfastqsdf), function(x){ paste0(paste(paste(rawfastqsdf[x , c("Prefix", "Read")], collapse = "_"), "primerremoved", sep = "_"), ".fastq.gz") } )
} else {
    rawfastqsdf$FilterInput_fn <- sapply(1:nrow(rawfastqsdf), function(x){ paste0(paste(paste(rawfastqsdf[x , c("Prefix", "Read")], collapse = "_"), "filt", sep = "_"), ".fastq.gz") } )
}

#Be double sure to have the same order always
rawfastqsdfR1 <- subset(rawfastqsdf, Read == "R1")
rownames(rawfastqsdfR1) <- rawfastqsdfR1$Prefix
rawfastqsdfR2 <- subset(rawfastqsdf, Read == "R2")
rownames(rawfastqsdfR2) <- rawfastqsdfR2$Prefix
rawfastqsdfR2 <- rawfastqsdfR2[rownames(rawfastqsdfR1) , ]

flog.info("Filtering and trimming reads")
if (!is.null(opt$forward_primer) && !is.null(opt$reverse_primer)) {

    if (!is.null(opt$SRAaccessions)){
        idfield <- 1
    } else {
        idfield <- NULL
    }

    multithread_primer_removal <- TRUE
    if (multithread_primer_removal){

        flog.info(paste("Removing forward primers using", max((opt$threads - 2), 1), "CPUs..."))
        dmp <- mclapply(1:nrow(rawfastqsdfR1), function (x) { removePrimers(fn = rawfastqsdfR1$Dada2_input_read_fpath[x], fout = rawfastqsdfR1$PrimerClippedFilePaths[x], primer.fwd = opt$forward_primer, trim.fwd = TRUE, trim.rev = FALSE, verbose = TRUE) }, mc.cores = max((opt$threads - 2), 1))

        flog.info(paste("Removing reverse primers using", max((opt$threads - 2), 1), "CPUs..."))
        dmp <- mclapply(1:nrow(rawfastqsdfR2), function (x) { removePrimers(fn = rawfastqsdfR2$Dada2_input_read_fpath[x], fout = rawfastqsdfR2$PrimerClippedFilePaths[x], primer.fwd = opt$reverse_primer, trim.fwd = TRUE, trim.rev = FALSE, verbose = TRUE) }, mc.cores = max((opt$threads - 2), 1))

    } else {

        flog.info("Removing forward primers...")
        removePrimers(fn = rawfastqsdfR1$Dada2_input_read_fpath, fout = rawfastqsdfR1$PrimerClippedFilePaths, primer.fwd = opt$forward_primer, trim.fwd = TRUE, trim.rev = FALSE, verbose = TRUE)

        flog.info("Removing reverse primers...")
        removePrimers(fn = rawfastqsdfR2$Dada2_input_read_fpath, fout = rawfastqsdfR2$PrimerClippedFilePaths, primer.fwd = opt$reverse_primer, trim.fwd = TRUE, trim.rev = FALSE, verbose = TRUE)

    }

    #Readjust truncation lengths, as these may have been shortened due to primer removal
    reassess_trunclengths <- function(trunclen_cutoffs_R1_R2 = NULL){
        fastq_dfs <- c("rawfastqsdfR1", "rawfastqsdfR2")
        new_trunclen_cutoffs_R1_R2 <- NULL
        for (tp in 1:length(trunclen_cutoffs_R1_R2)){
            curr_fqs <- countfastq_files(fastqfiles = get(fastq_dfs[tp])[]$PrimerClippedFilePaths, threads = opt$threads, type_tag = "NoPrim")
            min_length <- min(curr_fqs$Readlength)
            new_trunclen_cutoffs_R1_R2[tp] <- min(trunclen_cutoffs_R1_R2[tp], min_length)
        }

        return(new_trunclen_cutoffs_R1_R2)
    }
    trunclen_cutoffs_R1_R2 <- reassess_trunclengths(trunclen_cutoffs_R1_R2 = trunclen_cutoffs_R1_R2)
    flog.info(paste("Due to primer removal, truncation lengths are set to", paste0(trunclen_cutoffs_R1_R2, collapse = ", "), "bp."))

    flog.info("Filtering and trimming...")
    out <- filterAndTrim(fwd = rawfastqsdfR1$PrimerClippedFilePaths, filt = rawfastqsdfR1$FilteredFilePaths, rev = rawfastqsdfR2$PrimerClippedFilePaths, filt.rev = rawfastqsdfR2$FilteredFilePaths, truncLen = trunclen_cutoffs_R1_R2, compress = TRUE, truncQ = 2, maxN = 0, maxEE = c(2,2), rm.phix = TRUE, n = 1e+05, OMP = FALSE, verbose = TRUE, multithread = max((opt$threads - 2), 1), matchIDs = TRUE, id.field = idfield)
  
} else {

    out <- filterAndTrim(fwd = rawfastqsdfR1$Dada2_input_read_fpath, filt = rawfastqsdfR1$FilteredFilePaths, rev = rawfastqsdfR2$Dada2_input_read_fpath, filt.rev = rawfastqsdfR2$FilteredFilePaths, truncLen = trunclen_cutoffs_R1_R2, compress = TRUE, truncQ = 2, maxN = 0, maxEE = c(2,2), rm.phix = TRUE, n = 1e+05, OMP = FALSE, verbose = TRUE, multithread = max((opt$threads - 2), 1), matchIDs = TRUE, id.field = idfield)

}

#Adjust out dataframe
out <- as.data.frame(out)
out$FilterInput_fn <- rownames(out)
out <- left_join(out, rawfastqsdf[ , c("FilterInput_fn", "Prefix")], by = "FilterInput_fn")
colnames(out)[1:2] <- c("Number_Read_Pairs_Input", "Number_Read_Pairs_Output")
out$FilterInput_fn <- NULL
rawfastqsdf <- left_join(rawfastqsdf, out, by = "Prefix")

#Get a list of reads which do not survive read number threshold
samplestodiscard <- subset(out, Number_Read_Pairs_Output < opt$min_read_count_threshold)[]$Prefix
rawfastqsdf$Sample_Status <- "Sample_Kept_Read_Count_Sufficient"
if (length(samplestodiscard) > 0){
    rawfastqsdf$Sample_Status[which(rawfastqsdf$Prefix %in% samplestodiscard)] <- "Sample_Eliminated_Low_Read_Count"
    flog.warn("Samples", paste0(samplestodiscard, collapse = ", "), "were discarded for having less than", opt$read_count_threshold, "reads after filtering.")
    #Reset reads data frame as not containing reads to discard
    rawfastqsdfR1 <- subset(rawfastqsdfR1, !(Prefix %in% samplestodiscard))
    rawfastqsdfR2 <- subset(rawfastqsdfR2, !(Prefix %in% samplestodiscard))
}

Samples_passing_threshold <- intersect(rawfastqsdfR1$Prefix, rawfastqsdfR2$Prefix)

Sample2Depth <- out[ , c("Prefix", "Number_Read_Pairs_Output")]
colnames(Sample2Depth)[1] <- "Sample"

rm(out)

filtFs <- rawfastqsdfR1$FilteredFilePaths
filtRs <- rawfastqsdfR2$FilteredFilePaths

sample.names <- rawfastqsdfR1$Prefix
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

flog.info("Learning errors")
errF <- learnErrors(filtFs, multithread = max((opt$threads - 2), 1))
errR <- learnErrors(filtRs, multithread = max((opt$threads - 2), 1))

if (opt$plot_read_qualities){
    flog.info("Plotting errors")
    pdf(paste(paste(project, "Dada2_Error_Rate_Plots", sep = "_"), "pdf", sep = "."), paper = "a4r")
    plotErrors(errF, nominalQ = TRUE)
    plotErrors(errR, nominalQ = TRUE)
    dev.off()
}
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

flog.info("Dereplicating reads")
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

flog.info("Running dada2 on dereplicated reads")
dadaFs <- dada(derepFs, err = errF, multithread = max((opt$threads - 2), 1))
dadaRs <- dada(derepRs, err = errR, multithread = max((opt$threads - 2), 1))
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

flog.info("Assembling amplicon contigs (\"merging pairs\")")
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE)
#obtain mean amplicon lengths
median_amplicon_lengths <- as.data.frame(sapply(names(mergers), function (x) {obtain_dada2_amplicon_sizes(mergers_list = mergers, sample_name = x, fun_to_apply = "median") } ))
colnames(median_amplicon_lengths)[1] <- "Median_Amplicon_Length_bp"
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

flog.info("Building counts table")
seqtab <- makeSequenceTable(mergers)
flog.info("Removing chimeras")
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus", multithread = max((opt$threads - 2), 1), verbose = TRUE)
##Adding rownames to seqtab.nochim which we need for expvec later if there is only one sample
if (length(sample.names) == 1){
    rownames(seqtab.nochim) <- sample.names
}
IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

getN <- function(x) { sum(getUniques(x)) }
##this breaks here if there is only one sample so fixing it so one sample can run no problem
##Fixing track so that if there is only one sample things will still run
if (length(sample.names) == 1){
    track <- as.data.frame(cbind(getN(dadaFs), getN(dadaRs), getN(mergers), rowSums(seqtab.nochim)))
} else {
    track <- as.data.frame(cbind(sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim)))
}
colnames(track) <- c("DenoisedF", "DenoisedR", "Merged", "NonChim")
track$Prefix <- sample.names
rawfastqsdf <- left_join(rawfastqsdf, track, by = "Prefix")
flog.info("Exporting read stats")
write.xlsx(rawfastqsdf, file = paste0(opt$projectname, "_Reads_Statistics.xlsx"), colNames = TRUE, rowNames = FALSE, colWidths = "auto", borders = "all")

##########################
## Assigning Taxonomy ##
##########################
#Set training taxonomy depending for each method of taxonomy assignment
if (opt$taxamethod == "dada2"){
    #Account for two different versions of filenames depending on Silva dada2 release.
    opt$silva_trainset <- file.path(opt$taxonomy_database, sort(list.files(path = opt$taxonomy_database, pattern = "_train_set.fa.gz|Genus_trainset.fa.gz"))[1])
    opt$silva_species_assignment <- file.path(opt$taxonomy_database, sort(list.files(path = opt$taxonomy_database, pattern = "species_assignment_|_assignSpecies.fa.gz"))[1])

} else if (opt$taxamethod == "IDtaxa"){

    opt$trainingset <- sort(file.path(opt$taxonomy_database, list.files(path = opt$taxonomy_database, pattern = glob2rx("SILVA*.RData"))))[1]

} else if (opt$taxamethod == "dada2pr2"){

    opt$trainingset <- sort(file.path(opt$taxonomy_database, list.files(path = opt$taxonomy_database, pattern = glob2rx("pr*.fasta.gz"))))[1]

} else if (opt$taxamethod == "IDtaxapr2"){

    opt$trainingset <- sort(file.path(opt$taxonomy_database, list.files(path = opt$taxonomy_database, pattern = glob2rx("pr*.rds"))))[1]

}

## IDtaxa with Silva ##
if (opt$taxamethod %in% c("IDtaxa")){
    flog.info("Obtaining DNAstringset from ASVs")
    dnafrommyseq <- DNAStringSet(getSequences(seqtab.nochim))
    flog.info("Assigning taxonomy with IDtaxa down to Species taxlevel. Please be patient")
    load(opt$trainingset)
    taxids <- IdTaxa(dnafrommyseq, trainingSet, strand="both", threshold = 60, processors=NULL, verbose=TRUE)
    ranks <- c("domain", "phylum", "class", "order", "family", "genus", "species")
    #Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
    taxid <- t(sapply(taxids, function(x) {
            m <- match(ranks, x$rank)
            taxa <- x$taxon[m]
            taxa[startsWith(taxa, "unclassified_")] <- NA
            taxa
    }))
    colnames(taxid) <- ranks; rownames(taxid) <- getSequences(seqtab.nochim)

    flog.info("Building JAMS-style SummarizedExperiment")
    #n.b. Pump all these objects into opt to declutter workspace.
    opt$tt <- as.data.frame(taxid)
    #Fix names to capital letters
    names(opt$tt) <- c("Domain", "Phylum", "Class", "Order", "Family", "Genus", "Species")
    opt$tt$Kingdom <- opt$tt$Domain
    opt$tt <- opt$tt[ , c("Domain", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")]
    taxlvls <- c("Domain", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus")
    taxtags <- c("d__", "k__", "p__", "c__", "o__", "f__", "g__")

## IDtaxa with PR2 ##
} else if (opt$taxamethod == "IDtaxapr2"){
flog.info("Obtaining DNAstringset from ASVs")
    dnafrommyseq <- DNAStringSet(getSequences(seqtab.nochim))
    flog.info("Assigning taxonomy with IDtaxa down to Species taxlevel. Please be patient")
    trainingSet <- readRDS(opt$trainingset)
    taxids <- IdTaxa(dnafrommyseq, trainingSet, strand="both", threshold = 60, processors=NULL, verbose=TRUE)
    IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)
    #No more Phylum and instead there is Supergroup, Division and Subdivision
    ranks <- c("Root","Domain","Supergroup","Division","Subdivision","Class","Order","Family","Genus","Species")
    #Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
    n_seq <- length(taxids)
    df_rows <- list()

    for(i in 1:n_seq){
        taxonomy<- taxids[[i]]$taxon
        apndlength= 10-length(taxonomy)
        taxonomy <- c(taxonomy,rep("Unclassifed",apndlength))
        #confidence <- taxids[[i]]$confidence
        #apndlength= 10-length(confidence)
        #confidence <- c(confidence,rep("100",apndlength))
        df_rows[[i]] = data.frame(taxonomy)
    }
    taxid <- t(as.data.frame(df_rows))
    colnames(taxid) <- ranks; rownames(taxid) <- getSequences(seqtab.nochim)

    flog.info("Building JAMS-style SummarizedExperiment")
    #n.b. Pump all these objects into opt to declutter workspace.
    #Fix names to capital letters
    opt$tt <- as.data.frame(taxid)
    opt$tt$Kingdom <- opt$tt$Domain
    opt$tt <- opt$tt[ , c("Domain", "Kingdom", "Supergroup","Division","Subdivision", "Class", "Order", "Family", "Genus", "Species")]
    taxlvls <- c("Domain", "Kingdom", "Supergroup", "Division", "Subdivision","Class", "Order", "Family", "Genus")
    taxtags <- c("d__", "k__", "sg__", "dv__", "sd__", "c__", "o__", "f__", "g__", "s__")

## DADA2 with PR2 ##
} else if (opt$taxamethod == "dada2pr2"){
    #Using Dada2 with PR2
    flog.info("Assigning taxonomy down to Species taxlevel with pr2")
    #No more Phylum and instead there is Supergroup, Division and Subdivision
    taxa <- assignTaxonomy(seqtab.nochim, opt$trainingset, taxLevels = c("Domain","Supergroup","Division","Subdivision","Class","Order","Family","Genus","Species"), multithread = max((opt$threads - 2), 1))
    IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

    flog.info("Building JAMS-style SummarizedExperiment")
    #n.b. Pump all these objects into opt to declutter workspace.
    opt$tt <- as.data.frame(taxa)
    #Fix names to capital letters
    names(opt$tt) <- c("Domain", "Supergroup", "Division", "Subdivision","Class", "Order", "Family", "Genus", "Species")
    opt$tt$Kingdom <- opt$tt$Domain
    opt$tt <- opt$tt[ , c("Domain", "Kingdom", "Supergroup","Division","Subdivision", "Class", "Order", "Family", "Genus", "Species")]
    taxlvls <- c("Domain", "Kingdom", "Supergroup", "Division", "Subdivision","Class", "Order", "Family", "Genus", "Species")
    taxtags <- c("d__", "k__", "sg__", "dv__", "sd__", "c__", "o__", "f__", "g__", "s__")


} else if (opt$taxamethod == "dada2"){
## DADA2 with Silva ##
    flog.info("Assigning taxonomy down to Genus taxlevel")
    taxa <- assignTaxonomy(seqtab.nochim, opt$silva_trainset, multithread = max((opt$threads - 2), 1))
    flog.info("Assigning taxonomy down to Species level")
    taxa <- addSpecies(taxa, opt$silva_species_assignment)
    IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

    flog.info("Building JAMS-style SummarizedExperiment")
    #n.b. Pump all these objects into opt to declutter workspace.
    opt$tt <- as.data.frame(taxa)
    opt$tt$Domain <- opt$tt$Kingdom
    opt$tt <- opt$tt[ , c("Domain", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")]
    taxlvls <- c("Domain", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus")
    taxtags <- c("d__", "k__", "p__", "c__", "o__", "f__", "g__")
}

IO_jams_workspace_image(opt = opt, operation = "save", verbose = FALSE)

######################################
## Cleaning and Joining to Metadata ##
######################################

if (opt$taxamethod %in% c("dada2", "dada2pr2","IDtaxa")){
    for (lvl in 1:length(taxlvls)){
    taxlvl <- taxlvls[lvl]
    opt$tt[which(is.na(opt$tt[ , taxlvl]) == FALSE) , taxlvl] <- paste0(taxtags[lvl], opt$tt[which(is.na(opt$tt[ , taxlvl]) == FALSE) , taxlvl])
    opt$tt[which(is.na(opt$tt[ , taxlvl]) == TRUE) , taxlvl] <- "Unclassified"
    }
}

opt$tt[which(is.na(opt$tt[ , "Species"]) == TRUE) , "Species"] <- "Unclassified"
##Inferring LKT based on taxonomic database
if (opt$taxamethod %in% c("dada2","IDtaxa")){
    opt$tt <- infer_LKT(opt$tt)
    } else if (opt$taxamethod %in% c("dada2pr2","IDtaxapr2")){
    opt$tt <- infer_LKT_pr2(opt$tt)
    }

if (opt$taxamethod %in% c("IDtaxapr2")){
    for (lvl in 1:length(taxlvls)){
        taxlvl <- taxlvls[lvl]
        opt$tt[which(is.na(opt$tt[ , taxlvl]) == FALSE) , taxlvl] <- paste0(taxtags[lvl], opt$tt[which(is.na(opt$tt[ , taxlvl]) == FALSE) , taxlvl])
    }
}

opt$tt$Species <- opt$tt$LKT
opt$tt$LKT <- make.unique(opt$tt$LKT)

#Get counts table
opt$cts <- t(seqtab.nochim)
opt$cts <- as.data.frame(opt$cts)

#Maintain same order as cts
opt$tt <- opt$tt[rownames(opt$cts), ]

#Create ASV to LKT dictionary
asv2lkt <- data.frame(ASV = rownames(opt$tt), LKT = opt$tt$LKT, stringsAsFactors = FALSE)

#Call ASVs as non-redundant LKTs
rownames(opt$tt) <- opt$tt$LKT
rownames(opt$cts) <- opt$tt$LKT

rownames(opt$phenotable) <- opt$phenotable$Sample
pheno <- opt$phenotable

#Add sequencing depth
pheno <- left_join(pheno, Sample2Depth, by = "Sample")
rownames(pheno) <- pheno$Sample
#Add amplicon lengths
pheno$Median_Amplicon_Length_bp <- median_amplicon_lengths[rownames(pheno), "Median_Amplicon_Length_bp"]

#Pheno for more than one sample.
if (length(sample.names) > 1){
    pheno <- pheno[colnames(opt$cts), ]
}

variable_list <- define_kinds_of_variables(phenolabels = opt$phenolabels, phenotable = pheno, maxclass = 15, maxsubclass = 4, class_to_ignore = "N_A", verbose = TRUE)
ctable <- make_colour_dictionary(variable_list = variable_list, pheno = pheno, class_to_ignore = "N_A", colour_of_class_to_ignore = "#bcc2c2", colour_table = NULL, shuffle = FALSE, object_to_return = "ctable")

IO_jams_workspace_image(opt = opt, operation = "save", verbose = TRUE)

#Stop here and save if nothing else is required (if one sample only)
if (length(sample.names) == 1){
flog.info("Thank you for using JAMS16 for one sample")
    q()
}
##Otherwise build an expvec
expvec <- list()
expvec$Amplicon <- make_SummarizedExperiment_from_tables(pheno = pheno, counttable = opt$cts, featuretable = opt$tt, analysisname = "16S")

metadata(expvec$Amplicon)$asv2lkt <- asv2lkt
metadata(expvec$Amplicon)$ctable <- ctable

#Export to system as a courtesy to the user
saveRDS(expvec$Amplicon, file = paste(paste(project, "Amplicon_SummarizedExperiment_object", sep = "_"), "rds", sep = "."))

#######
## Clean up workspace image to reduce RAM footprint
objects_to_clean <- c("dadaFs", "dadaRs", "derepFs", "derepRs", "mergers", "quality_list", "quality_list_tmp", "seqtab", "seqtab.nochim", "taxa", "asv2lkt", "colm", "errF", "errR", "filtFs", "filtRs", "fls")
objects_to_clean <- objects_to_clean[objects_to_clean %in% ls()]
rm(list = objects_to_clean)
gc()

flog.info(paste0("JAMS16 run of project ", opt$project, " is complete. Thank you for using JAMS."))
flog.info(print_jams_ASCII())

#Save image
IO_jams_workspace_image(opt = opt, operation = "save", verbose = TRUE)
q()
